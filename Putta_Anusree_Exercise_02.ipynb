{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnusreePutta/Info-5731/blob/main/Putta_Anusree_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Metrics for employee productivity:\n",
        "\n",
        "Make use of key performance indicators (KPIs) like work output, task achievement, and project completion rates.\n",
        "Compile information on hours worked, deadlines fulfilled, and project milestones attained.\n",
        "\n",
        "Surveys of job satisfaction:\n",
        "\n",
        "Conduct surveys on a regular basis to find out how satisfied staff members are working remotely.\n",
        "Ask about workplace satisfaction, communication effectiveness, and work-life balance.\n",
        "Evaluations of well-being:\n",
        "\n",
        "Conduct interviews or questionnaires to evaluate the physical and emotional health of your staff.\n",
        "Include inquiries about general health perceptions, anxiety related to job, and stress levels.\n",
        "Metrics for Collaboration and Communication:\n",
        "\n",
        "Collaborative actions, communication frequency, and communication technologies utilized should all be recorded.\n",
        "Examine data pertaining to online conferences, chat conversations, and group document editing.\n",
        "Metrics for employee productivity:\n",
        "\n",
        "Make use of key performance indicators (KPIs) like work output, task achievement, and project completion rates.\n",
        "Compile information on hours worked, deadlines fulfilled, and project milestones attained.\n",
        "\n",
        "Procedures for Gathering and Preserving Data:\n",
        "\n",
        "Metrics for Productivity:\n",
        "\n",
        "Combine time monitoring software with project management tools to get productivity metrics in real time.\n",
        "Assign the data to employee identification and safely store it in a centralized database.\n",
        "Surveys of job satisfaction:\n",
        "\n",
        "To conduct surveys on a regular basis, use specialized tools or online survey platforms.\n",
        "Survey replies should be safely stored while preserving participant confidentiality.\n",
        "Evaluations of well-being:\n",
        "\n",
        "Conduct interviews or surveys, then document the answers.\n",
        "Maintain participant privacy by securely storing well-being data.\n",
        "Metrics for Collaboration and Communication:\n",
        "\n",
        "Take information out of communication and teamwork platforms like Zoom, Slack, and Microsoft Teams.\n",
        "Assign time periods and employee IDs to the metrics and store them in a safe database.\n",
        "Patterns of Technology Usage:\n",
        "\n",
        "Make use of the logs from the platforms' APIs or remote work technologies.\n",
        "Save information about technology use safely."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Create a synthetic dataset\n",
        "def generate_synthetic_data(num_samples=1000):\n",
        "    data = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        age = random.randint(18, 35)\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        education_background = random.choice(['High School', 'Undergraduate', 'Graduate'])\n",
        "        prior_experience_vr = random.choice(['Yes', 'No'])\n",
        "\n",
        "        baseline_engagement = random.randint(1, 5)\n",
        "        post_engagement = random.randint(1, 5)\n",
        "\n",
        "        heart_rate = random.randint(60, 100)\n",
        "        skin_conductance = random.uniform(1.5, 8.0)\n",
        "        eye_movement = random.uniform(0.5, 3.0)\n",
        "\n",
        "        learning_outcome_score = random.uniform(50, 100)\n",
        "        observer_rating = random.randint(1, 5)\n",
        "\n",
        "        vr_usage_frequency = random.randint(1, 10)\n",
        "        vr_usage_duration = random.uniform(5, 120)\n",
        "\n",
        "        data.append([age, gender, education_background, prior_experience_vr,\n",
        "                     baseline_engagement, post_engagement,\n",
        "                     heart_rate, skin_conductance, eye_movement,\n",
        "                     learning_outcome_score, observer_rating,\n",
        "                     vr_usage_frequency, vr_usage_duration])\n",
        "\n",
        "    columns = ['Age', 'Gender', 'Education Background', 'Prior Experience with VR',\n",
        "               'Baseline Engagement', 'Post-Engagement',\n",
        "               'Heart Rate', 'Skin Conductance', 'Eye Movement',\n",
        "               'Learning Outcome Score', 'Observer Rating',\n",
        "               'VR Usage Frequency', 'VR Usage Duration']\n",
        "\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    return df\n",
        "\n",
        "# Generate synthetic dataset with 1000 samples\n",
        "dataset = generate_synthetic_data(num_samples=1000)\n",
        "\n",
        "# Save dataset to a CSV file\n",
        "dataset.to_csv('vr_educational_impact_dataset.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a77499-94be-44fa-c8fd-d163caeeef6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 600 articles and saved to 'articles.json'.\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Function to fetch articles from Google Scholar\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []  # List to store the collected articles\n",
        "\n",
        "    # Loop to paginate through search results (10 results per page)\n",
        "    for start in range(0, num_articles, 10):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result\n",
        "            for result in results:\n",
        "                article = {}  # Dictionary to store article information\n",
        "\n",
        "                # Extract the title of the article (inside an h3 element with class 'gs_rt')\n",
        "                title = result.find('h3', {'class': 'gs_rt'})\n",
        "                if title:\n",
        "                    article['title'] = title.text  # Store the title in the dictionary\n",
        "\n",
        "                # Extract the venue/journal/conference information (inside a div element with class 'gs_a')\n",
        "                venue = result.find('div', {'class': 'gs_a'})\n",
        "                if venue:\n",
        "                    article['venue'] = venue.text  # Store the venue information\n",
        "\n",
        "                # Extract the publication year (from the 'gs_a' div)\n",
        "                year = result.find('div', {'class': 'gs_a'})\n",
        "                if year:\n",
        "                    # Split the text and get the last part (usually the year), then strip whitespace\n",
        "                    year = year.text.split('-')[-1].strip()\n",
        "                    article['year'] = year  # Store the year in the dictionary\n",
        "\n",
        "                # Extract the authors (from the 'gs_a' div)\n",
        "                authors = result.find('div', {'class': 'gs_a'})\n",
        "                if authors:\n",
        "                    # Split the text and get the first part (usually the authors), then strip whitespace\n",
        "                    authors = authors.text.split('-')[0].strip()\n",
        "                    article['authors'] = authors  # Store the authors in the dictionary\n",
        "\n",
        "                # Extract the abstract (inside a div element with class 'gs_rs')\n",
        "                abstract = result.find('div', {'class': 'gs_rs'})\n",
        "                if abstract:\n",
        "                    article['abstract'] = abstract.text  # Store the abstract in the dictionary\n",
        "\n",
        "                # Append the article dictionary to the list of articles\n",
        "                articles.append(article)\n",
        "\n",
        "                # Check if the desired number of articles has been collected\n",
        "                if len(articles) >= num_articles:\n",
        "                    return articles\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2013                # Start year of publication range\n",
        "    end_year = 2023                  # End year of publication range\n",
        "    num_articles = 1000              # Desired number of articles to collect\n",
        "\n",
        "    # Call the fetch_google_scholar_articles function to collect articles\n",
        "    articles = fetch_google_scholar_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "    # Save the collected articles to a JSON file\n",
        "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(articles, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Print the number of collected articles and a confirmation message\n",
        "    print(f\"Collected {len(articles)} articles and saved to 'articles.json'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "77e15e0e-7291-48d3-ba97-8ab54b84be26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'API' object has no attribute 'search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a0085c5535ec>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Example: Collect tweets with the keyword 'Python'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtwitter_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Display the collected data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a0085c5535ec>\u001b[0m in \u001b[0;36mcollect_tweets\u001b[0;34m(keyword, num_tweets)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtweets_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
          ]
        }
      ],
      "source": [
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = 'your_consumer_key'\n",
        "consumer_secret = 'your_consumer_secret'\n",
        "access_token = 'your_access_token'\n",
        "access_token_secret = 'your_access_token_secret'\n",
        "\n",
        "# Set up Tweepy with the credentials\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# Function to collect tweets based on a keyword\n",
        "def collect_tweets(keyword, num_tweets=100):\n",
        "    tweets_data = []\n",
        "    for tweet in tweepy.Cursor(api.search, q=keyword, lang='en').items(num_tweets):\n",
        "        tweets_data.append([tweet.id_str, tweet.created_at, tweet.user.screen_name, tweet.text])\n",
        "\n",
        "    columns = ['Tweet ID', 'Created At', 'Username', 'Text']\n",
        "    df = pd.DataFrame(tweets_data, columns=columns)\n",
        "    return df\n",
        "\n",
        "# Example: Collect tweets with the keyword 'Python'\n",
        "twitter_df = collect_tweets('Python', num_tweets=100)\n",
        "\n",
        "# Display the collected data\n",
        "print(twitter_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I57NXsauCec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ea332b13-56b2-4454-c55c-4bb6f6b5cf1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'API' object has no attribute 'search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-03bc251b6dcc>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Example: Collect tweets with the keyword 'Python'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Display the collected data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-03bc251b6dcc>\u001b[0m in \u001b[0;36mcollect_tweets\u001b[0;34m(keyword, num_tweets)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tweets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtweets_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtweets_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
          ]
        }
      ],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''Procedures for Gathering and Preserving Data:\n",
        "\n",
        "Metrics for Productivity:\n",
        "\n",
        "Combine time monitoring software with project management tools to get productivity metrics in real time.\n",
        "Assign the data to employee identification and safely store it in a centralized database.\n",
        "Surveys of job satisfaction:\n",
        "\n",
        "To conduct surveys on a regular basis, use specialized tools or online survey platforms.\n",
        "Survey replies should be safely stored while preserving participant confidentiality.\n",
        "Evaluations of well-being:\n",
        "\n",
        "Conduct interviews or surveys, then document the answers.\n",
        "Maintain participant privacy by securely storing well-being data.\n",
        "Metrics for Collaboration and Communication:\n",
        "\n",
        "Take information out of communication and teamwork platforms like Zoom, Slack, and Microsoft Teams.\n",
        "Assign time periods and employee IDs to the metrics and store them in a safe database.\n",
        "Patterns of Technology Usage:\n",
        "\n",
        "Make use of the logs from the platforms' APIs or remote work technologies.\n",
        "Save information about technology use safely."
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}