{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnusreePutta/Info-5731/blob/main/Putta_Anusree_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "I am considering sentimental analysis for this answer.sentimental analysis on social media comments to say wheather comments are positive,negative or neutral.\n",
        "The 5 points which I considered are:\n",
        "\n",
        "1.Bag-of-words:\n",
        "Explanation: BoW is a measure of a word's frequency in a document that takes its occurrence into account rather than its order. This sums up the entire vocabulary that was used in the comment.\n",
        "Usefulness: The frequency of words might aid the model in identifying sentiment patterns, as certain phrases may be linked to different sentiments.\n",
        "\n",
        "2.Term Frequency-Inverse Document Frequency, or TF-IDF:\n",
        "\n",
        "To put it another way, TF-IDF is a statistical metric that assesses a word's significance in a document in relation to a corpus of documents.\n",
        "Usefulness: Words that appear frequently in a particular comment but infrequently in the corpus as a whole may reflect sentiment. Words that are strongly connected to either happiness or negativity, for instance.\n",
        "\n",
        "3.Analysis of Emotions:\n",
        "\n",
        "Explanation: Examining verbally expressed emotions like happiness, rage, grief, etc.\n",
        "Usefulness: Specific sentiments are linked to particular emotions. For example, happy feelings frequently involve happiness, whereas negative feelings could involve despair or wrath.\n",
        "\n",
        "4.Tags for Part-of-Speech (POS):\n",
        "\n",
        "Explaination: Determining which grammatical category—nouns, verbs, adjectives, etc.—each word belongs in.\n",
        "Usefulness: Sentences' syntactic composition can be understood by looking at the grammatical structure, which can help reveal sentiment's complexities and nuances.\n",
        "\n",
        "N-grams:\n",
        "\n",
        "Definition: A document's sequence of 'n' adjacent words, where 'n' can be one, two, three, or more.\n",
        "Contextual information is captured, which aids in comprehending the meaning of words in relation to their neighbors. Bigrams or trigrams, for instance, may capture particular words that express emotion.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "!python -m nltk.downloader averaged_perceptron_tagger\n",
        "\n",
        "# Sample dataset\n",
        "reviews = [\n",
        "    (\"Positive review\", \"This product is excellent, and I am satisfied with my purchase.\"),\n",
        "    (\"Negative review\", \"The quality of the product is disappointing, and I regret buying it.\"),\n",
        "    (\"Neutral review\", \"The product is okay, not too bad but not outstanding either.\")\n",
        "]\n",
        "\n",
        "# Extracting features\n",
        "labels, texts = zip(*reviews)\n",
        "\n",
        "# Bag-of-Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "bow_matrix = vectorizer_bow.fit_transform(texts)\n",
        "print(\"\\nBag-of-Words (BoW):\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(bow_matrix.toarray())\n",
        "\n",
        "# TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer_tfidf.fit_transform(texts)\n",
        "print(\"\\nTF-IDF:\")\n",
        "print(vectorizer_tfidf.get_feature_names_out())\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiments = [sia.polarity_scores(text) for text in texts]\n",
        "print(\"\\nSentiment Analysis:\")\n",
        "print(sentiments)\n",
        "\n",
        "# POS Tags\n",
        "pos_tags = [nltk.pos_tag(word_tokenize(text.lower())) for text in texts]\n",
        "print(\"\\nPart-of-Speech (POS) Tags:\")\n",
        "print(pos_tags)\n",
        "\n",
        "# N-grams\n",
        "n_grams_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "n_grams_matrix = n_grams_vectorizer.fit_transform(texts)\n",
        "print(\"\\nN-grams:\")\n",
        "print(n_grams_vectorizer.get_feature_names_out())\n",
        "print(n_grams_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebCzYCwlLxJ4",
        "outputId": "36a2ce52-9de0-43a0-defd-5c356f88e184"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "\n",
            "Bag-of-Words (BoW):\n",
            "['am' 'and' 'bad' 'but' 'buying' 'disappointing' 'either' 'excellent' 'is'\n",
            " 'it' 'my' 'not' 'of' 'okay' 'outstanding' 'product' 'purchase' 'quality'\n",
            " 'regret' 'satisfied' 'the' 'this' 'too' 'with']\n",
            "[[1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1]\n",
            " [0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 1 0 1 1 0 2 0 0 0]\n",
            " [0 0 1 1 0 0 1 0 1 0 0 2 0 1 1 1 0 0 0 0 1 0 1 0]]\n",
            "\n",
            "TF-IDF:\n",
            "['am' 'and' 'bad' 'but' 'buying' 'disappointing' 'either' 'excellent' 'is'\n",
            " 'it' 'my' 'not' 'of' 'okay' 'outstanding' 'product' 'purchase' 'quality'\n",
            " 'regret' 'satisfied' 'the' 'this' 'too' 'with']\n",
            "[[0.34760686 0.26436398 0.         0.         0.         0.\n",
            "  0.         0.34760686 0.20530221 0.         0.34760686 0.\n",
            "  0.         0.         0.         0.20530221 0.34760686 0.\n",
            "  0.         0.34760686 0.         0.34760686 0.         0.34760686]\n",
            " [0.         0.24559104 0.         0.         0.3229227  0.3229227\n",
            "  0.         0.         0.19072335 0.3229227  0.         0.\n",
            "  0.3229227  0.         0.         0.19072335 0.         0.3229227\n",
            "  0.3229227  0.         0.49118207 0.         0.         0.        ]\n",
            " [0.         0.         0.29779776 0.29779776 0.         0.\n",
            "  0.29779776 0.         0.17588415 0.         0.         0.59559551\n",
            "  0.         0.29779776 0.29779776 0.17588415 0.         0.\n",
            "  0.         0.         0.22648287 0.         0.29779776 0.        ]]\n",
            "\n",
            "Sentiment Analysis:\n",
            "[{'neg': 0.0, 'neu': 0.581, 'pos': 0.419, 'compound': 0.7579}, {'neg': 0.375, 'neu': 0.625, 'pos': 0.0, 'compound': -0.7184}, {'neg': 0.276, 'neu': 0.509, 'pos': 0.215, 'compound': -0.4506}]\n",
            "\n",
            "Part-of-Speech (POS) Tags:\n",
            "[[('this', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('excellent', 'JJ'), (',', ','), ('and', 'CC'), ('i', 'VB'), ('am', 'VBP'), ('satisfied', 'VBN'), ('with', 'IN'), ('my', 'PRP$'), ('purchase', 'NN'), ('.', '.')], [('the', 'DT'), ('quality', 'NN'), ('of', 'IN'), ('the', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('disappointing', 'JJ'), (',', ','), ('and', 'CC'), ('i', 'NN'), ('regret', 'VBP'), ('buying', 'VBG'), ('it', 'PRP'), ('.', '.')], [('the', 'DT'), ('product', 'NN'), ('is', 'VBZ'), ('okay', 'JJ'), (',', ','), ('not', 'RB'), ('too', 'RB'), ('bad', 'JJ'), ('but', 'CC'), ('not', 'RB'), ('outstanding', 'JJ'), ('either', 'RB'), ('.', '.')]]\n",
            "\n",
            "N-grams:\n",
            "['am' 'am satisfied' 'and' 'and am' 'and regret' 'bad' 'bad but' 'but'\n",
            " 'but not' 'buying' 'buying it' 'disappointing' 'disappointing and'\n",
            " 'either' 'excellent' 'excellent and' 'is' 'is disappointing'\n",
            " 'is excellent' 'is okay' 'it' 'my' 'my purchase' 'not' 'not outstanding'\n",
            " 'not too' 'of' 'of the' 'okay' 'okay not' 'outstanding'\n",
            " 'outstanding either' 'product' 'product is' 'purchase' 'quality'\n",
            " 'quality of' 'regret' 'regret buying' 'satisfied' 'satisfied with' 'the'\n",
            " 'the product' 'the quality' 'this' 'this product' 'too' 'too bad' 'with'\n",
            " 'with my']\n",
            "[[1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0\n",
            "  0 0 0 1 1 0 0 0 1 1 0 0 1 1]\n",
            " [0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1\n",
            "  1 1 1 0 0 2 1 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 2 1 1 0 0 1 1 1 1 1 1 0 0\n",
            "  0 0 0 0 0 1 1 0 0 0 1 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiYKUOrIPR0x",
        "outputId": "65de9059-d686-41c7-b8a5-72c1489d783f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "0MOueeefPTqI"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc574b8-edb6-4acb-d848-f0a6ad8933e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Rank 1: Similarity = 0.7423\n",
            "Review: We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.\n",
            "\n",
            "Rank 2: Similarity = 0.7376\n",
            "Review: Validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks\n",
            "\n",
            "Rank 3: Similarity = 0.7193\n",
            "Review: Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches\n",
            "\n",
            "Rank 4: Similarity = 0.7073\n",
            "Review: As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI)\n",
            "\n",
            "Rank 5: Similarity = 0.6945\n",
            "Review: Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models\n",
            "\n",
            "Rank 6: Similarity = 0.6818\n",
            "Review: a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI.\n",
            "\n",
            "Rank 7: Similarity = 0.6588\n",
            "Review: Making it an avant-garde platform for pioneering new prompting strategies This research aims to bridge the gap between LLM reasoning and human thought processes, mirroring the intricate networks observed in phenomena like brain mechanisms and recurrence.\n",
            "\n",
            "Rank 8: Similarity = 0.5997\n",
            "Review: further democratizing access in this important, fast-changing field.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "from torchmetrics.functional import cosine_similarity\n",
        "# Sample text data\n",
        "text_data = [\n",
        "    \"As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI)\",\n",
        "    \"a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI.\",\n",
        "    \"Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models\",\n",
        "    \"further democratizing access in this important, fast-changing field.\",\n",
        "    \"Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches\",\n",
        "    \"Validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks\",\n",
        "    \"We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.\",\n",
        "    \"Making it an avant-garde platform for pioneering new prompting strategies This research aims to bridge the gap between LLM reasoning and human thought processes, mirroring the intricate networks observed in phenomena like brain mechanisms and recurrence.\",\n",
        "]\n",
        "\n",
        "# Query\n",
        "query = \"I'm looking for a reliable Llama model for text summarization.\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize and encode the query\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "   query_output = model(**tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")).last_hidden_state.mean(dim=1)\n",
        "# Calculate cosine similarity between the query and each review\n",
        "# Calculate cosine similarity between the query and each review\n",
        "similarities = [cosine_similarity(query_output.detach(), model(**tokenizer(review, padding=True, truncation=True, return_tensors=\"pt\")).last_hidden_state.mean(dim=1)).item() for review in text_data]\n",
        "# Rank the reviews based on similarity in descending order\n",
        "ranked_reviews = sorted(zip(text_data, similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the ranked reviews\n",
        "for i, (review, similarity) in enumerate(ranked_reviews, start=1):\n",
        "    print(f\"Rank {i}: Similarity = {similarity:.4f}\\nReview: {review}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Experience: I learned a lot from working on the text data extraction and feature engineering assignments. Tokenization, different feature extraction techniques like Bag-of-Words and TF-IDF, and sophisticated approaches like Word Embeddings (Word2Vec), Topic Modeling (LDA), Named Entity Recognition (NER), and Sentiment Analysis are some of the important ideas that stood out. For NLP tasks, it is essential to know how to preprocess text data and extract relevant features.\n",
        "\n",
        "Challenges Faced: Although the exercises addressed a wide range of features and strategies, the code snippets' simplification for clarity may provide a challenge. In real-world applications, the choice of parameters, model fine-tuning, and managing larger datasets might create extra complications. Additionally, using cutting-edge models like BERT may necessitate some experience with deep"
      ],
      "metadata": {
        "id": "obnzH0rdip32"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}